{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "To prove that $VC_{dim}(H)=\\infty$ it's necessary to demonstrate that $\\forall m\\ \\exists S:\\ |S|=m$ which can be shattered by $H$. \n",
    "\n",
    "According to the \"The Nature of Statistical Learning Theory\" (Vapnik, 1995, p. 82) let's take $S(m) =\\{x: x_{i} = 10^{-i}, \\ i\\in[1...m]  \\}$ and $a = \\pi(\\sum_{i=1}^{l}(1-y_{i})10^i + 1) $, where $y_{i}$ is a number from the set $\\{0, 1 \\}$\n",
    "\n",
    "Let's take  $x_{j}$ and prove that our classifier can correctly predict its label i.e. prove that $sgn(sin(ax_{j}))$ depends only on $x_{j}$ label:\n",
    "$$f_a(x_{j}) = sng(sin( \\pi(\\sum_{i=1}^{l}(1-y_{i})10^{i-j} + 10^{-j})))$$\n",
    "When $i>j$  under the sum we have $10^{k}, k \\in N$. After multipluing by $\\pi$ we get: $$f_a(x_{j}) = sgn(sin(10^{k}\\pi+\\pi\\sum_{i=1}^{j}(1-y_{})10^{i-j} + 10^{-j}\\pi)) = sgn(sin(\\pi\\sum_{i=1}^{j}(1-y_{i})10^{i-j} + 10^{-j}\\pi)) = sgn(sin(\\pi\\sum_{i=0}^{j-1}(1-y_{i})10^{i-j} + (1-y_j)\\pi)), \\text{where} \\quad y_{i} = 0 $$  \n",
    "Let's estimate sums under the sin by the sum of geometric progression:\n",
    "$$\\pi\\sum_{i=0}^{j-1}(1-y_{i})10^{i-j} \\le \\pi\\sum_{i=0}^{j-1}10^{i-j} = \\pi\\frac{10^{1}10^{-1}- 10^{-j}}{10-1}=\\pi\\frac{1- 10^{-j}}{9}<\\frac{\\pi}{9}$$\n",
    "Now, in deppendace of $x_{j}$ label we get different sign under the sin because of presence or absence the last member $\\pi$:\n",
    "$$\\pi\\sum_{i=0}^{j-1}(1-y_{i})10^{i-j} + (1-y_j)\\pi) \\in[(1-y_j)\\pi,\\pi+(1-y_j)\\pi]$$\n",
    "So if we have $y_{j} = 1$: $sgn(sin([0,\\pi]))>0 $, else if $y_{j} = 0$: $sgn(sin([\\pi,2\\pi]))<0$\n",
    "\n",
    "So our family $H_a$ can shatter the set $S$ of any size $m$, $VC_{dim}(H)=\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "\n",
    "Let's suppose that we have this architecture with 2 neurons in each layer with the depth $L=1$. So the output (after applying nonlinear function) of linear combinations of that functions could produce a new family of functions. This family could consist of initial ones and other functions like $f_{a,b}(x) = 1_{[a,b]}(x)$ and $f_{a,b}(x) = 0_{[a,b]}(x)$. So in the process of propogating through the network in each layer we produce a bigger family of functions.\n",
    "\n",
    "<img src=\"https://pp.userapi.com/c847221/v847221700/12eda9/1GCy7LIalzU.jpg\" alt=\"Drawing\" style=\"height: 900px;\"# title=”this text appears when mouse or finger hovers over image” />\n",
    "\n",
    "Now we brought our network to $sgn(sin(ax))$ approximation with a finite number of peaks. So our task is reduced to the task of defining the function of $L$ with which $VC_{dim}\\to\\infty$. Let's make a tough estimate that with such a this network we can't shatter a subset of $2(2^{L-2})$ poins. That means that we get the upper bound of $VC_{dim} \\le 2^{L-1}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
